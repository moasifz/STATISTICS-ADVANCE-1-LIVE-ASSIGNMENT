{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#1-Expalin the properties of the F-distribution. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-Properties of the F-distribution:The F-distribution is used in analysis of variance and hypothesis testing and has the following properties.\n#Positive Skewed:The distribution is not symmetric,skewed to the right.\n#Non-negative Values:The F-distribution only takes values greater than or equal to zero.\n#Dependent on Degrees of Freedom: Its shape is determined by two sets of degrees of freedom:one for the numerator and one for the denominator.\n#Mean:The mean of the F-distribution is approximately 1 when the degrees of freedom in the numeartor and denominator are equal.\n#Relation to Chi-Squared Distribution: The F-distribution can be derived from the ratio of two scaled chi-squared distribution.\n#specifically,if X and Y are independent\n#chi-squared variables with d1 and d2 degrees of freedom,respectively,the the ratio X/d1/Y/d2 follows an F-distribution.\n#Critical values:F-distribution tables or software are often used to find critical values for hypothesis testing.\n#The critical value depends on the significance level(alpha) and the degrees of freedom.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "#2-In which types of statistical tests is the F-distribution used,and why is it appropriate for these tests?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-The F-distribution is primarily used in several types of statistical tests, particularly those involving comparisons of variances or means across multiple groups.\n#Here are the main contexts in which it is used:\n#1-ANOVA (Analysis of Variance):\n#Purpose: ANOVA tests whether there are statistically significant differences between the means of three or more groups.\n#Appropriateness: It compares the variance between group means to the variance within the groups. The ratio of these variances follows an F-distribution under the null hypothesis that all group means are equal.\n\n#2-Regression Analysis:\n\n#Purpose: In multiple regression, the F-test is used to determine if at least one predictor variable has a non-zero coefficient, indicating a significant relationship with the response variable.\n#Appropriateness: The F-statistic is the ratio of the explained variance (due to the regression model) to the unexplained variance (error). This ratio follows an F-distribution.\n\n#3-Comparing Variances:\n#Purpose: Tests like Bartlett's test and Levene's test evaluate the equality of variances across different groups.\n#Appropriateness: The F-distribution is used to assess the ratio of variances, as the test statistics for these tests are derived from variances of different groups.\n\n#4-Nested Models:\n\n#Purpose: When comparing nested models in regression, an F-test can determine if the more complex model significantly improves the fit.\n#Appropriateness: The F-statistic assesses the improvement in variance explained by the additional parameters in the more complex model compared to the simpler model.\n\n#5-Quality Control:\n#Purpose: In control charts and process variability assessments, the F-test can compare variances from different production processes or groups.\n#Appropriateness: This helps determine if process changes or conditions result in significant differences in variability.\n# The F-distribution is appropriate because it tests hypotheses that involve comparing variances,and variance naturally takes non-negative values.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "#3-What are the key assumptions required for conducting an F-test to compare the variances of two populations.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-When conducting an F-test to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test results. These assumptions include:\n\n#1-Independence:\n#The samples drawn from the two populations must be independent of each other\n#This means that the selection of one sample does not influence the selection of the other.\n\n#2-Normality:\n#Both populations should be normally distributed\n#While the F-test is somewhat robust to violations of normality, especially with large sample sizes,\n#significant deviations from normality can affect the results, particularly with smaller samples.\n\n#3-Homogeneity of Variance (this is what you are testing):\n#The variances of the two populations being compared should be approximately equal under the null hypothesis.\n#The F-test specifically evaluates this assumption by comparing the ratio of the sample variances.\n\n#4-Random Sampling:\n#The samples should be randomly selected from their respective populations.\n#This helps ensure that the samples are representative and that the results can be generalized.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "#4-What is the purpose of ANNOVA,and how does it differ from a t-test? ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-Purpose of ANOVA\n#ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups to determine,\n#if there are any statistically significant differences among them.\n#The main purposes of ANOVA are:\n\n#1-Testing for Differences: ANOVA assesses whether at least one group mean is different from the others,\n#helping to identify which treatments or conditions have varying effects.\n\n#2-Controlling Type I Error: When comparing multiple groups, using multiple t-tests increases the risk of Type I error (incorrectly rejecting the null hypothesis).\n#ANOVA controls for this by evaluating all group means simultaneously.\n\n#3-Understanding Variance: ANOVA partitions the total variance observed in the data into variance due to group differences and variance due to random error,\n#providing insights into the sources of variability.\n\n#  Differences Between ANOVA and t-Test\n# 1-Number of Groups:\n#--ANOVA: Used when comparing three or more groups.\n#--t-Test: Typically used when comparing the means of two groups.\n\n# 2-Hypotheses:\n#--ANOVA: Tests the null hypothesis that all group means are equal.\n#--t-Test: Tests the null hypothesis that two group means are equal.\n\n# 3-Control of Type I Error:\n#ANOVA: Controls for Type I error across multiple comparisons by providing a single test for all groups.\n#t-Test: If multiple t-tests are performed, the risk of Type I error increases unless adjustments (like the Bonferroni correction) are made.\n\n# 4-Output:\n#ANOVA: Provides an F-statistic to evaluate variance between groups relative to variance within groups.\n#t-Test: Provides a t-statistic to evaluate the difference between two means.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "#5-Explain when and why you would use a one-way ANNOVA instead of multiple t-tests when comparing more than two groups.  ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-Using a one-way ANOVA instead of multiple t-tests when comparing more than two groups is recommended for several important reasons:\n#--When to Use One-Way ANOVA:\n#1-Comparing Three or More Groups: One-way ANOVA is appropriate when you have three or more groups to compare.\n#-If you want to assess differences in means across these groups, ANOVA is the suitable choice.\n\n#--Why Use One-Way ANOVA Instead of Multiple t-Tests\n#1-Control of Type I Error Rate:\n#--Multiple Comparisons Problem: Each t-test carries a risk of making a Type I error (rejecting a true null hypothesis).\n#-When conducting multiple t-tests, the cumulative probability of incorrectly rejecting at least one null hypothesis increases.\n#-One-way ANOVA maintains the overall Type I error rate at the desired significance level (e.g., 0.05).\n#2-Efficiency:\n#-Single Test: One-way ANOVA allows you to compare all group means simultaneously in a single test. This is more efficient than performing multiple t-tests,\n#-which require separate calculations and adjustments.\n#-3-Variability Assessment:\n#-Partitioning Variance: One-way ANOVA provides a comprehensive analysis by partitioning total variance into variance between groups and within groups.\n#-This helps in understanding the sources of variability more clearly.\n#4-Overall Hypothesis Testing:\n#-General Null Hypothesis: One-way ANOVA tests the null hypothesis that all group means are equal. If the null is rejected,\n#-you can then follow up with post-hoc tests to determine which specific groups differ, providing a structured approach to hypothesis testing.\n#5-Post-Hoc Testing:\n#Follow-Up Analysis: If the one-way ANOVA indicates significant differences, you can conduct post-hoc tests (e.g., Tukeyâ€™s HSD, Bonferroni)\n#to determine which pairs of groups differ, rather than making multiple pairwise comparisons directly.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "#6-Explain how variance is partitioned in ANNOVA into-between-group variance and within-group variance.How does this partitioning contribute to the calculation of the F-statistic?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-In ANOVA (Analysis of Variance), variance is partitioned into two main components: between-group variance and within-group variance.\n#This partitioning helps to assess whether the means of different groups are significantly different from one another.\n#1- Between-Group Variance\n# Definition: Between-group variance measures how much the group means deviate from the overall mean of all groups combined.\n#Calculation:\n\n#-First, calculate the overall mean of all observations.\n#-For each group, compute the mean and then calculate the squared differences between each group mean and the overall mean.\n#-Weight these squared differences by the number of observations in each group.\n\n#.2- Within-Group Variance\n# Definition: Within-group variance measures how much the individual observations within each group deviate from their respective group mean.\n#Calculation:\n\n#For each observation, calculate the squared difference between the observation and its group mean.\n#Sum these squared differences across all groups.\n\n#Partitioning Contribution to F-statistic\n#The F-statistic is calculated as the ratio of the mean square between groups to the mean square within groups:\n# F = MSB/MSW\n# Where: MSB = SSB/dfB(mean square between groups,with dfB = g - 1).\n# MSW = SSw/dfw(mean square within groups with dfw = N - g,where N is the total number of observations).\n\n#The f-statistics is calculated by dividing the between-group variance by the within-group variance.A high F-value suggests significant group diffrences.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "#7-Compare the classical(frequentist) approach to ANNOVA with the Bayesian approach.What are the key diffrences in terms of how they handle uncertainty,parameter estimation,and hypothesis testing?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-The classical (frequentist) approach to ANOVA and the Bayesian approach differ significantly in how they handle uncertainty,\n#parameter estimation, and hypothesis testing. Hereâ€™s a comparison of the key differences.\n#1-Handling Uncertainty\n# Frequentist Approach:\n#-Uncertainty is often expressed in terms of confidence intervals and p-values. The confidence interval gives a range of values that,\n#-with a certain level of confidence (e.g., 95%), is believed to contain the true parameter.\n#-P-values indicate the probability of observing the data (or something more extreme) given that the null hypothesis is true.\n#Bayesian Approach:\n#Uncertainty is quantified using probability distributions. Instead of a single estimate, Bayesian methods provide a full posterior\n#distribution for parameters after observing data.\n#This distribution reflects both prior beliefs and the evidence from the data, allowing for a more intuitive understanding of uncertainty.\n\n#2-Parameter Estimation\n#  Frequentist Approach:\n#Parameters are typically estimated using point estimates (e.g., means) and are considered fixed values. Estimators have properties such as unbiasedness and consistency.\n#The estimation process does not incorporate prior beliefs; it relies solely on the observed data.\n#  Bayesian Approach:\n#Parameters are treated as random variables with prior distributions. The posterior distribution combines the prior and the likelihood from the data.\n#This allows for more flexibility in incorporating prior knowledge and updating beliefs as new data becomes available.\n\n#3- Hypothesis Testing\n# Frequentist Approach:\n#Hypothesis testing is done through a null hypothesis (often that all group means are equal) \n#and involves calculating a p-value. A p-value below a chosen significance level (e.g., 0.05) leads to rejection of the null hypothesis.\n#Frequentist methods do not provide the probability that a hypothesis is true; they focus on the data given the hypothesis.\n# Bayesian Approach:\n#Bayesian hypothesis testing allows for direct probability statements about hypotheses.\n#You can compute the probability of the null hypothesis given the data (using Bayes' theorem).\n#Bayesian methods often use Bayes factors to compare the evidence for two competing hypotheses, \n#allowing for a more nuanced understanding of the data's implications.\n\n#Bayesian methods handle uncertainty more naturally,providing a full distribution of possible outcomes,\n#whereas classical methods focus on point estimates and confidence intervals.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "#8-Questions:You have two sets of data representing the incomes of two different professions:\nProfession A:[48,52,55,60,62]\nProfession B:[45,50,55,52,47]\nPerform an F-test to determine if the variances of the two professions incomes are equal.What are your conclusions based on the F-test?\nTask:Use Python to calculate the F-statistic and p-value for the given data.\nObjective:Gain experience in performing F-tests and interpreting the results in terms of variance comparison.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-To perform an F-test to determine if the variances of the incomes from two different professions are equal,\n# I can use Python. Here to calculate the F-statistic and p-value.\n#Step-by-Step \n#1-Calculate the variances for both sets of data.\n#2-Compute the F-statistic as the ratio of the variances.\n#3-Determine the p-value based on the F-distribution.\n\n# Here is how i perform the F-test using Python:\n\nimport numpy as np\nimport scipy.stats as stats\n\n# Data for the two professions\nprofession_a = np.array([48, 52, 55, 60, 62])\nprofession_b = np.array([45, 50, 55, 52, 47])\n\n# Calculate variances\nvar_a = np.var(profession_a, ddof=1)  # Sample variance\nvar_b = np.var(profession_b, ddof=1)  # Sample variance\n\n# Calculate the F-statistic\nf_statistic = var_a / var_b\n\n# Degrees of freedom\ndf_a = len(profession_a) - 1\ndf_b = len(profession_b) - 1\n\n# Calculate the p-value\np_value = 1 - stats.f.cdf(f_statistic, df_a, df_b)\n\n# Print results\nprint(f\"Variance of Profession A: {var_a:.2f}\")\nprint(f\"Variance of Profession B: {var_b:.2f}\")\nprint(f\"F-statistic: {f_statistic:.2f}\")\nprint(f\"P-value: {p_value:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Variance of Profession A: 32.80\nVariance of Profession B: 15.70\nF-statistic: 2.09\nP-value: 0.2465\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "#Output explanation:When i run this code,we will see the variances of both professions,\n#the calculated F-statistic, and the corresponding p-value.\n\n# Interpretation of Results\n#1-F-statistic: This is the ratio of the variances. A higher F-statistic suggests that the variance of Profession A is greater than that of Profession B.\n#2-P-value: This value tells you the probability of observing the data if the null hypothesis (that the variances are equal) is true.\n# If the p-value is less than your significance level (commonly 0.05), you reject the null hypothesis,\n#indicating that the variances are significantly different.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "#9-Questions:Conduct a one-way ANNOVA to test whether there are any statistaclly significant differences in average heights between three different regions with the following data:\nRegion A:[160,162,165,158,164]\nRegion B:[172,175,170,168,174]\nRegion C:[180,182,179,185,183]\nTask:Write Python code to perform the one-way ANNOVA and interpret the results.\nObjective:Learn how to perform one-way ANNOVA using Python and interpret F-statistic and p-value.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ANS-To conduct a one-way ANOVA to test for statistically significant differences in average heights between three different regions,\n#I use Pythonâ€™s scipy.stats.\n#Below is the step-by-step and code to perform the one-way ANOVA and interpret the results.\n#Data\n#Region A: [160, 162, 165, 158, 164]\n#Region B: [172, 175, 170, 168, 174]\n#Region C: [180, 182, 179, 185, 183]\n\n#python code\nimport numpy as np\nimport scipy.stats as stats\n\n# Data for the three regions\nregion_a = np.array([160, 162, 165, 158, 164])\nregion_b = np.array([172, 175, 170, 168, 174])\nregion_c = np.array([180, 182, 179, 185, 183])\n\n# Perform one-way ANOVA\nf_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n\n# Print results\nprint(f\"F-statistic: {f_statistic:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "F-statistic: 67.8733\nP-value: 0.0000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "#Interpretation of Results\n#1-F-statistic: This value indicates the ratio of the variance between the groups to the variance within the groups.\n#A higher F-statistic suggests a greater disparity between group means compared to the variability within the groups.\n\n#2-P-value: This value indicates the probability of observing the data, or something more extreme,\n#if the null hypothesis (that all group means are equal) is true.\n#If the p-value is less than your significance level (commonly set at 0.05), you reject the null hypothesis,\n#indicating that there are statistically significant differences in average heights among the regions.\n\n#Here is how we interpret them:\n#If the p-value < 0.05: there are statistically significant differences in average heights between at least two of the regions.\n#If the p-value > 0.05: there is not enough evidence to suggest significant differences in average heights between the regions.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}